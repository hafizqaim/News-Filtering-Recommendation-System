{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37e08d2a",
   "metadata": {},
   "source": [
    "# **PHASE # 1: TRAINING**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9694e658",
   "metadata": {},
   "source": [
    "## **Required Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d6c199-a992-4435-b3ba-8a6f361adeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pyarrow\n",
    "import builtins\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torch\n",
    "from nltk.corpus import stopwords\n",
    "from annoy import AnnoyIndex\n",
    "import faiss\n",
    "from datasets import Dataset\n",
    "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling, BitsAndBytesConfig\n",
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662922e4",
   "metadata": {},
   "source": [
    "## **Loading the Datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0aa1f8fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news_id</th>\n",
       "      <th>category</th>\n",
       "      <th>subcategory</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>url</th>\n",
       "      <th>title_entities</th>\n",
       "      <th>abstract_entities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>N55528</td>\n",
       "      <td>lifestyle</td>\n",
       "      <td>lifestyleroyals</td>\n",
       "      <td>The Brands Queen Elizabeth, Prince Charles, an...</td>\n",
       "      <td>Shop the notebooks, jackets, and more that the...</td>\n",
       "      <td>https://assets.msn.com/labs/mind/AAGH0ET.html</td>\n",
       "      <td>[{\"Label\": \"Prince Philip, Duke of Edinburgh\",...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>N19639</td>\n",
       "      <td>health</td>\n",
       "      <td>weightloss</td>\n",
       "      <td>50 Worst Habits For Belly Fat</td>\n",
       "      <td>These seemingly harmless habits are holding yo...</td>\n",
       "      <td>https://assets.msn.com/labs/mind/AAB19MK.html</td>\n",
       "      <td>[{\"Label\": \"Adipose tissue\", \"Type\": \"C\", \"Wik...</td>\n",
       "      <td>[{\"Label\": \"Adipose tissue\", \"Type\": \"C\", \"Wik...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>N61837</td>\n",
       "      <td>news</td>\n",
       "      <td>newsworld</td>\n",
       "      <td>The Cost of Trump's Aid Freeze in the Trenches...</td>\n",
       "      <td>Lt. Ivan Molchanets peeked over a parapet of s...</td>\n",
       "      <td>https://assets.msn.com/labs/mind/AAJgNsz.html</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{\"Label\": \"Ukraine\", \"Type\": \"G\", \"WikidataId...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>N53526</td>\n",
       "      <td>health</td>\n",
       "      <td>voices</td>\n",
       "      <td>I Was An NBA Wife. Here's How It Affected My M...</td>\n",
       "      <td>I felt like I was a fraud, and being an NBA wi...</td>\n",
       "      <td>https://assets.msn.com/labs/mind/AACk2N6.html</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{\"Label\": \"National Basketball Association\", ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>N38324</td>\n",
       "      <td>health</td>\n",
       "      <td>medical</td>\n",
       "      <td>How to Get Rid of Skin Tags, According to a De...</td>\n",
       "      <td>They seem harmless, but there's a very good re...</td>\n",
       "      <td>https://assets.msn.com/labs/mind/AAAKEkt.html</td>\n",
       "      <td>[{\"Label\": \"Skin tag\", \"Type\": \"C\", \"WikidataI...</td>\n",
       "      <td>[{\"Label\": \"Skin tag\", \"Type\": \"C\", \"WikidataI...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  news_id   category      subcategory  \\\n",
       "0  N55528  lifestyle  lifestyleroyals   \n",
       "1  N19639     health       weightloss   \n",
       "2  N61837       news        newsworld   \n",
       "3  N53526     health           voices   \n",
       "4  N38324     health          medical   \n",
       "\n",
       "                                               title  \\\n",
       "0  The Brands Queen Elizabeth, Prince Charles, an...   \n",
       "1                      50 Worst Habits For Belly Fat   \n",
       "2  The Cost of Trump's Aid Freeze in the Trenches...   \n",
       "3  I Was An NBA Wife. Here's How It Affected My M...   \n",
       "4  How to Get Rid of Skin Tags, According to a De...   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  Shop the notebooks, jackets, and more that the...   \n",
       "1  These seemingly harmless habits are holding yo...   \n",
       "2  Lt. Ivan Molchanets peeked over a parapet of s...   \n",
       "3  I felt like I was a fraud, and being an NBA wi...   \n",
       "4  They seem harmless, but there's a very good re...   \n",
       "\n",
       "                                             url  \\\n",
       "0  https://assets.msn.com/labs/mind/AAGH0ET.html   \n",
       "1  https://assets.msn.com/labs/mind/AAB19MK.html   \n",
       "2  https://assets.msn.com/labs/mind/AAJgNsz.html   \n",
       "3  https://assets.msn.com/labs/mind/AACk2N6.html   \n",
       "4  https://assets.msn.com/labs/mind/AAAKEkt.html   \n",
       "\n",
       "                                      title_entities  \\\n",
       "0  [{\"Label\": \"Prince Philip, Duke of Edinburgh\",...   \n",
       "1  [{\"Label\": \"Adipose tissue\", \"Type\": \"C\", \"Wik...   \n",
       "2                                                 []   \n",
       "3                                                 []   \n",
       "4  [{\"Label\": \"Skin tag\", \"Type\": \"C\", \"WikidataI...   \n",
       "\n",
       "                                   abstract_entities  \n",
       "0                                                 []  \n",
       "1  [{\"Label\": \"Adipose tissue\", \"Type\": \"C\", \"Wik...  \n",
       "2  [{\"Label\": \"Ukraine\", \"Type\": \"G\", \"WikidataId...  \n",
       "3  [{\"Label\": \"National Basketball Association\", ...  \n",
       "4  [{\"Label\": \"Skin tag\", \"Type\": \"C\", \"WikidataI...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df = pd.read_csv('MINDsmall_train/news.tsv', sep='\\t', header=None)\n",
    "\n",
    "news_df.columns = [\n",
    "    \"news_id\", \"category\", \"subcategory\", \"title\", \"abstract\",\n",
    "    \"url\", \"title_entities\", \"abstract_entities\"\n",
    "]\n",
    "\n",
    "behaviors_df = pd.read_csv('MINDsmall_train/behaviors.tsv', sep='\\t', header=None)\n",
    "\n",
    "behaviors_df.columns = [\n",
    "    \"impression_id\", \"user_id\", \"timestamp\", \"history\", \"impressions\"\n",
    "]\n",
    "\n",
    "news_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "398f5c5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>impression_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>history</th>\n",
       "      <th>impressions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>U13740</td>\n",
       "      <td>11/11/2019 9:05:58 AM</td>\n",
       "      <td>N55189 N42782 N34694 N45794 N18445 N63302 N104...</td>\n",
       "      <td>N55689-1 N35729-0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>U91836</td>\n",
       "      <td>11/12/2019 6:11:30 PM</td>\n",
       "      <td>N31739 N6072 N63045 N23979 N35656 N43353 N8129...</td>\n",
       "      <td>N20678-0 N39317-0 N58114-0 N20495-0 N42977-0 N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>U73700</td>\n",
       "      <td>11/14/2019 7:01:48 AM</td>\n",
       "      <td>N10732 N25792 N7563 N21087 N41087 N5445 N60384...</td>\n",
       "      <td>N50014-0 N23877-0 N35389-0 N49712-0 N16844-0 N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>U34670</td>\n",
       "      <td>11/11/2019 5:28:05 AM</td>\n",
       "      <td>N45729 N2203 N871 N53880 N41375 N43142 N33013 ...</td>\n",
       "      <td>N35729-0 N33632-0 N49685-1 N27581-0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>U8125</td>\n",
       "      <td>11/12/2019 4:11:21 PM</td>\n",
       "      <td>N10078 N56514 N14904 N33740</td>\n",
       "      <td>N39985-0 N36050-0 N16096-0 N8400-1 N22407-0 N6...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   impression_id user_id              timestamp  \\\n",
       "0              1  U13740  11/11/2019 9:05:58 AM   \n",
       "1              2  U91836  11/12/2019 6:11:30 PM   \n",
       "2              3  U73700  11/14/2019 7:01:48 AM   \n",
       "3              4  U34670  11/11/2019 5:28:05 AM   \n",
       "4              5   U8125  11/12/2019 4:11:21 PM   \n",
       "\n",
       "                                             history  \\\n",
       "0  N55189 N42782 N34694 N45794 N18445 N63302 N104...   \n",
       "1  N31739 N6072 N63045 N23979 N35656 N43353 N8129...   \n",
       "2  N10732 N25792 N7563 N21087 N41087 N5445 N60384...   \n",
       "3  N45729 N2203 N871 N53880 N41375 N43142 N33013 ...   \n",
       "4                        N10078 N56514 N14904 N33740   \n",
       "\n",
       "                                         impressions  \n",
       "0                                  N55689-1 N35729-0  \n",
       "1  N20678-0 N39317-0 N58114-0 N20495-0 N42977-0 N...  \n",
       "2  N50014-0 N23877-0 N35389-0 N49712-0 N16844-0 N...  \n",
       "3                N35729-0 N33632-0 N49685-1 N27581-0  \n",
       "4  N39985-0 N36050-0 N16096-0 N8400-1 N22407-0 N6...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "behaviors_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4fd214e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 156965 entries, 0 to 156964\n",
      "Data columns (total 5 columns):\n",
      " #   Column         Non-Null Count   Dtype \n",
      "---  ------         --------------   ----- \n",
      " 0   impression_id  156965 non-null  int64 \n",
      " 1   user_id        156965 non-null  object\n",
      " 2   timestamp      156965 non-null  object\n",
      " 3   history        153727 non-null  object\n",
      " 4   impressions    156965 non-null  object\n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 6.0+ MB\n"
     ]
    }
   ],
   "source": [
    "behaviors_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31ddbd08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 51282 entries, 0 to 51281\n",
      "Data columns (total 8 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   news_id            51282 non-null  object\n",
      " 1   category           51282 non-null  object\n",
      " 2   subcategory        51282 non-null  object\n",
      " 3   title              51282 non-null  object\n",
      " 4   abstract           48616 non-null  object\n",
      " 5   url                51282 non-null  object\n",
      " 6   title_entities     51279 non-null  object\n",
      " 7   abstract_entities  51278 non-null  object\n",
      "dtypes: object(8)\n",
      "memory usage: 3.1+ MB\n"
     ]
    }
   ],
   "source": [
    "news_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f702e668",
   "metadata": {},
   "source": [
    "## **Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "316414c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing title/abstract with empty strings\n",
    "news_df['title'] = news_df['title'].fillna('')\n",
    "news_df['abstract'] = news_df['abstract'].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4eae25cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine title and abstract into a single 'text' column\n",
    "news_df['text'] = news_df['title'] + \". \" + news_df['abstract']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96b319f9-2143-4b26-b6e2-1bffae8e8d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicate articles based on the combined text\n",
    "news_df = news_df.drop_duplicates(subset='text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6104c015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows where 'text' is empty or just whitespace\n",
    "news_df = news_df[news_df['text'].str.strip() != '']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de82bfa",
   "metadata": {},
   "source": [
    "## **Splitting the Data into Train and Test Sets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2f91c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Splitting Datasets into 80/20 Train/Test ---\n",
      "News Train Data Shape: (40535, 9)\n",
      "News Test Data Shape: (10134, 9)\n",
      "Behaviors Train Data Shape: (125634, 5)\n",
      "Behaviors Test Data Shape: (31331, 5)\n",
      "✅ Test datasets saved to 'MINDsmall_test/'\n",
      "--- Dataset Splitting Completed ---\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Limiting Data to First 10,000 Rows ---\")\n",
    "\n",
    "# Limit news_df to first 10,000 rows\n",
    "# Using .copy() to ensure we're working on a separate DataFrame to avoid SettingWithCopyWarning\n",
    "news_df_limited = news_df.head(10000).copy().reset_index(drop=True)\n",
    "print(f\"Limited News Data Shape: {news_df_limited.shape}\")\n",
    "\n",
    "# Limit behaviors_df to first 10,000 rows\n",
    "behaviors_df_limited = behaviors_df.head(10000).copy().reset_index(drop=True)\n",
    "print(f\"Limited Behaviors Data Shape: {behaviors_df_limited.shape}\")\n",
    "\n",
    "\n",
    "print(\"\\n--- Splitting Limited Datasets into 80/20 Train/Test ---\")\n",
    "\n",
    "# Split news_df_limited (articles)\n",
    "# IMPORTANT: Removed 'stratify=news_df_limited['category']' to prevent ValueError\n",
    "# as small subsets might have categories with only 1 member.\n",
    "news_df_train, news_df_test = train_test_split(news_df_limited, test_size=0.2, random_state=42)\n",
    "# Reset index to ensure continuous indexing for embedding lookup later\n",
    "news_df_train = news_df_train.reset_index(drop=True)\n",
    "news_df_test = news_df_test.reset_index(drop=True)\n",
    "\n",
    "print(f\"News Train Data Shape (from 10k): {news_df_train.shape}\")\n",
    "print(f\"News Test Data Shape (from 10k): {news_df_test.shape}\")\n",
    "\n",
    "# Split behaviors_df_limited (user interactions) by user_id\n",
    "# This ensures that users are distinct between train and test sets for evaluation.\n",
    "unique_users_limited = behaviors_df_limited['user_id'].unique()\n",
    "train_users, test_users = train_test_split(unique_users_limited, test_size=0.2, random_state=42)\n",
    "\n",
    "behaviors_df_train = behaviors_df_limited[behaviors_df_limited['user_id'].isin(train_users)].reset_index(drop=True)\n",
    "behaviors_df_test = behaviors_df_limited[behaviors_df_limited['user_id'].isin(test_users)].reset_index(drop=True)\n",
    "\n",
    "print(f\"Behaviors Train Data Shape (from 10k): {behaviors_df_train.shape}\")\n",
    "print(f\"Behaviors Test Data Shape (from 10k): {behaviors_df_test.shape}\")\n",
    "\n",
    "# It's good practice to save the test sets for later use in Phase 2 (Inference/Evaluation)\n",
    "# Using a new directory name for clarity, indicating it's a 10k subset.\n",
    "try:\n",
    "    os.makedirs('MINDsmall_test_10k_subset', exist_ok=True)\n",
    "    news_df_test.to_parquet('MINDsmall_test_10k_subset/news_test_10k.parquet', index=False)\n",
    "    behaviors_df_test.to_parquet('MINDsmall_test_10k_subset/behaviors_test_10k.parquet', index=False)\n",
    "    print(\"✅ Limited Test datasets saved to 'MINDsmall_test_10k_subset/'\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Could not save limited test datasets: {e}. Please ensure directory exists.\")\n",
    "\n",
    "print(\"--- Dataset Splitting Completed ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabc9772",
   "metadata": {},
   "source": [
    "## **Add Validation Split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ba860f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New section for splitting data into actual train/validation sets\n",
    "print(\"\\n--- Creating Validation Split from Training Data (80/20 for actual train/val) ---\")\n",
    "\n",
    "# Split news_df_train for actual training and validation articles\n",
    "# (Assuming news_df_train is already created from the 80/20 split of the 10k subset)\n",
    "news_df_actual_train, news_df_val = train_test_split(news_df_train, test_size=0.2, random_state=42)\n",
    "news_df_actual_train = news_df_actual_train.reset_index(drop=True)\n",
    "news_df_val = news_df_val.reset_index(drop=True)\n",
    "\n",
    "print(f\"News Actual Train Data Shape: {news_df_actual_train.shape}\")\n",
    "print(f\"News Validation Data Shape: {news_df_val.shape}\")\n",
    "\n",
    "# Split behaviors_df_train for actual training and validation user interactions\n",
    "# Use user-based split for consistency and proper evaluation.\n",
    "# (Assuming behaviors_df_train is already created from the 80/20 split of the 10k subset)\n",
    "unique_users_train = behaviors_df_train['user_id'].unique()\n",
    "actual_train_users, val_users = train_test_split(unique_users_train, test_size=0.2, random_state=42)\n",
    "\n",
    "behaviors_df_actual_train = behaviors_df_train[behaviors_df_train['user_id'].isin(actual_train_users)].reset_index(drop=True)\n",
    "behaviors_df_val = behaviors_df_train[behaviors_df_train['user_id'].isin(val_users)].reset_index(drop=True)\n",
    "\n",
    "print(f\"Behaviors Actual Train Data Shape: {behaviors_df_actual_train.shape}\")\n",
    "print(f\"Behaviors Validation Data Shape: {behaviors_df_val.shape}\")\n",
    "\n",
    "print(\"--- Validation Split Completed ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb31b7c",
   "metadata": {},
   "source": [
    "## **Add Evaluation Metrics Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3674362",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math # Ensure math is imported globally\n",
    "\n",
    "def get_true_relevant_items_val(behaviors_df_val):\n",
    "    true_relevant_items_dict = {}\n",
    "    for _, row in behaviors_df_val.iterrows():\n",
    "        user_id = row['user_id']\n",
    "        if pd.isna(row['impressions']): continue\n",
    "        relevant_items = [item.split('-')[0] for item in row['impressions'].split() if item.endswith('1')]\n",
    "        if relevant_items:\n",
    "            true_relevant_items_dict[user_id] = relevant_items\n",
    "    return true_relevant_items_dict\n",
    "\n",
    "def get_user_history_val(behaviors_df_val):\n",
    "    user_history_dict = {}\n",
    "    for _, row in behaviors_df_val.iterrows():\n",
    "        user_id = row['user_id']\n",
    "        if pd.isna(row['history']): continue\n",
    "        history_items = row['history'].split()\n",
    "        if history_items:\n",
    "            user_history_dict[user_id] = history_items\n",
    "    return user_history_dict\n",
    "\n",
    "def calculate_precision_recall_ndcg_map(recommended_items_dict, true_relevant_items_dict, k=10):\n",
    "    \"\"\"\n",
    "    Calculates Precision@k, Recall@k, NDCG@k, and MAP for a set of recommendations.\n",
    "\n",
    "    Args:\n",
    "        recommended_items_dict (dict): Dictionary mapping user_id to a list of recommended news_ids (ranked).\n",
    "        true_relevant_items_dict (dict): Dictionary mapping user_id to a list of true relevant news_ids.\n",
    "        k (int): The cutoff for top-k recommendations.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing average Precision@k, Recall@k, NDCG@k, and MAP.\n",
    "    \"\"\"\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    ndcgs = []\n",
    "    maps = []\n",
    "\n",
    "    for user_id in true_relevant_items_dict.keys():\n",
    "        if user_id not in recommended_items_dict or not recommended_items_dict[user_id]:\n",
    "            continue\n",
    "\n",
    "        recs = recommended_items_dict[user_id][:k]\n",
    "        true_relevants = set(true_relevant_items_dict[user_id])\n",
    "\n",
    "        # Precision@k\n",
    "        num_relevant_in_k = len(set(recs) & true_relevants)\n",
    "        precision_at_k = num_relevant_in_k / k if k > 0 else 0\n",
    "        precisions.append(precision_at_k)\n",
    "\n",
    "        # Recall@k\n",
    "        recall_at_k = num_relevant_in_k / len(true_relevants) if len(true_relevants) > 0 else 0\n",
    "        recalls.append(recall_at_k)\n",
    "\n",
    "        # NDCG@k\n",
    "        dcg = 0.0\n",
    "        idcg = 0.0\n",
    "        for i, item_id in enumerate(recs):\n",
    "            if item_id in true_relevants:\n",
    "                dcg += 1.0 / math.log2(i + 2) # i+1 is rank, so i+2 for log2\n",
    "\n",
    "        ideal_relevances = sorted([1.0 for _ in true_relevants], reverse=True) # Assume relevance is 1 for relevant items\n",
    "        for i, relevance in enumerate(ideal_relevances[:k]):\n",
    "            idcg += relevance / math.log2(i + 2)\n",
    "\n",
    "        ndcg_at_k = dcg / idcg if idcg > 0 else 0.0\n",
    "        ndcgs.append(ndcg_at_k)\n",
    "\n",
    "        # Average Precision (for MAP)\n",
    "        current_precision_sum = 0.0\n",
    "        num_hits = 0\n",
    "        for i, item_id in enumerate(recs):\n",
    "            if item_id in true_relevants:\n",
    "                num_hits += 1\n",
    "                current_precision_sum += num_hits / (i + 1)\n",
    "        \n",
    "        avg_precision = current_precision_sum / len(true_relevants) if len(true_relevants) > 0 else 0.0\n",
    "        maps.append(avg_precision)\n",
    "\n",
    "\n",
    "    avg_precision_at_k = np.mean(precisions) if precisions else 0\n",
    "    avg_recall_at_k = np.mean(recalls) if recalls else 0\n",
    "    avg_ndcg_at_k = np.mean(ndcgs) if ndcgs else 0\n",
    "    avg_map = np.mean(maps) if maps else 0\n",
    "\n",
    "    return {\n",
    "        f\"Precision@{k}\": avg_precision_at_k,\n",
    "        f\"Recall@{k}\": avg_recall_at_k,\n",
    "        f\"NDCG@{k}\": avg_ndcgs,\n",
    "        f\"MAP@{k}\": avg_map\n",
    "    }\n",
    "\n",
    "def simulate_recommendations_for_validation(user_history_dict, embedding_model, tokenizer_model=None, faiss_index=None, news_df_for_val=None, k=10):\n",
    "    \"\"\"\n",
    "    Simulates recommendations for a set of users based on their history during validation.\n",
    "    Handles both SentenceTransformer and CLM (DeepSeek) models for user embedding generation.\n",
    "\n",
    "    Args:\n",
    "        user_history_dict (dict): Dictionary mapping user_id to list of historical news_ids.\n",
    "        embedding_model: The SentenceTransformer or DeepSeek model instance.\n",
    "        tokenizer_model: The tokenizer for CLM models (needed for DeepSeek).\n",
    "        faiss_index: The Faiss index of article embeddings (should be from news_df_val).\n",
    "        news_df_for_val (pd.DataFrame): The news DataFrame used for validation embeddings.\n",
    "        k (int): Number of top recommendations to retrieve.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary mapping user_id to a list of recommended news_ids.\n",
    "    \"\"\"\n",
    "    if faiss_index is None or news_df_for_val is None:\n",
    "        raise ValueError(\"faiss_index and news_df_for_val must be provided for validation simulation.\")\n",
    "\n",
    "    recommended_items = {}\n",
    "    news_id_to_idx = {news_id: idx for idx, news_id in enumerate(news_df_for_val['news_id'].tolist())}\n",
    "    idx_to_news_id = {idx: news_id for news_id, idx in news_id_to_idx.items()}\n",
    "\n",
    "    # Ensure model is in evaluation mode\n",
    "    embedding_model.eval()\n",
    "\n",
    "    for user_id, history_news_ids in tqdm(user_history_dict.items(), desc=\"Simulating Val Recs\"):\n",
    "        if not history_news_ids:\n",
    "            continue\n",
    "\n",
    "        # Get embeddings for historical articles\n",
    "        history_texts = [news_df_for_val[news_df_for_val['news_id'] == nid]['text'].iloc[0]\n",
    "                         for nid in history_news_ids if nid in news_df_for_val['news_id'].values]\n",
    "\n",
    "        if not history_texts:\n",
    "            continue\n",
    "\n",
    "        user_embedding = None\n",
    "        with torch.no_grad(): # Ensure no gradient calculation for inference\n",
    "            if isinstance(embedding_model, SentenceTransformer):\n",
    "                history_embeddings = embedding_model.encode(history_texts, convert_to_numpy=True)\n",
    "                user_embedding = np.mean(history_embeddings, axis=0, keepdims=True)\n",
    "            elif isinstance(embedding_model, AutoModelForCausalLM) and tokenizer_model:\n",
    "                # DeepSeek user embedding generation using its pooling logic\n",
    "                all_history_embeddings = []\n",
    "                for text in history_texts:\n",
    "                    inputs = tokenizer_model(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(embedding_model.device)\n",
    "                    outputs = embedding_model(**inputs, output_hidden_states=True)\n",
    "                    # Corrected typo here: outputs.hidden_hidden_states -> outputs.hidden_states\n",
    "                    last_hidden_states = outputs.hidden_states[-1]\n",
    "                    attention_mask = inputs['attention_mask'].unsqueeze(-1).expand(last_hidden_states.size()).float()\n",
    "                    masked_embeddings = last_hidden_states * attention_mask\n",
    "                    sum_embeddings = torch.sum(masked_embeddings, 1)\n",
    "                    sum_mask = torch.sum(attention_mask, 1)\n",
    "                    mean_pooled_embedding = (sum_embeddings / torch.clamp(sum_mask, min=1e-9)).cpu().numpy()\n",
    "                    all_history_embeddings.append(mean_pooled_embedding)\n",
    "                \n",
    "                if all_history_embeddings: # Check if any embeddings were generated\n",
    "                    user_embedding = np.mean(np.vstack(all_history_embeddings), axis=0, keepdims=True)\n",
    "            \n",
    "            if user_embedding is None or (user_embedding.ndim == 1 and user_embedding.size == 0): # Handle empty user_embedding\n",
    "                continue # Skip if no user embedding could be generated\n",
    "\n",
    "            # Ensure user_embedding is a 2D array [1, dim] for faiss.normalize_L2\n",
    "            if user_embedding.ndim == 1:\n",
    "                user_embedding = np.expand_dims(user_embedding, axis=0)\n",
    "\n",
    "            faiss.normalize_L2(user_embedding)\n",
    "\n",
    "            # Search Faiss index for top-k\n",
    "            # Retrieve more to filter out already seen items\n",
    "            D, I = faiss_index.search(user_embedding, k + len(history_news_ids) + 5) # +5 buffer for safety\n",
    "            \n",
    "            # Filter out already seen items from recommendations\n",
    "            seen_items = set(history_news_ids)\n",
    "            # Ensure idx is valid before lookup and not seen\n",
    "            top_k_indices = [idx for idx in I[0] if idx >= 0 and idx < len(idx_to_news_id) and idx_to_news_id.get(idx) and idx_to_news_id.get(idx) not in seen_items]\n",
    "            \n",
    "            recommended_news_ids = [idx_to_news_id[idx] for idx in top_k_indices[:k]]\n",
    "            recommended_items[user_id] = recommended_news_ids\n",
    "\n",
    "    return recommended_items\n",
    "\n",
    "# --- Plotting helper function (can be placed here or after all training is done) ---\n",
    "def plot_training_metrics(results, model_name, metric_k=10):\n",
    "    epochs = range(1, len(results[\"train_losses\"]) + 1)\n",
    "\n",
    "    # Plot Training and Validation Loss Curves\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, results[\"train_losses\"], label='Training Loss')\n",
    "    plt.title(f'{model_name} - Training Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot Training Time per Epoch\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.bar(epochs, results[\"epoch_times\"])\n",
    "    plt.title(f'{model_name} - Training Time per Epoch')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Time (seconds)')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot Validation Accuracy Curves (Precision@k, Recall@k, NDCG@k, MAP@k)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(epochs, results[\"val_precisions\"], label=f'Validation Precision@{metric_k}')\n",
    "    plt.plot(epochs, results[\"val_recalls\"], label=f'Validation Recall@{metric_k}')\n",
    "    plt.plot(epochs, results[\"val_ndcgs\"], label=f'Validation NDCG@{metric_k}')\n",
    "    plt.plot(epochs, results[\"val_maps\"], label=f'Validation MAP@{metric_k}')\n",
    "    plt.title(f'{model_name} - Validation Metrics over Epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Score')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15af283",
   "metadata": {},
   "source": [
    "## **FINE TUNING**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab63b272",
   "metadata": {},
   "source": [
    "### **Create Triplets for Fine Tuning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1fd30b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_triplets(behaviors_df, news_dict, max_triplets=10000):\n",
    "    triplets = []\n",
    "    for _, row in behaviors_df.iterrows():\n",
    "        if pd.isna(row[\"impressions\"]): continue\n",
    "        impressions = row[\"impressions\"].split()\n",
    "        pos = [i.split('-')[0] for i in impressions if i.endswith('1')]\n",
    "        neg = [i.split('-')[0] for i in impressions if i.endswith('0')]\n",
    "        if len(pos) < 2 or not neg: continue\n",
    "        for i in range(len(pos)):\n",
    "            a, p = pos[i], pos[(i+1)%len(pos)]\n",
    "            for n in neg:\n",
    "                triplets.append((news_dict.get(a), news_dict.get(p), news_dict.get(n)))\n",
    "        if len(triplets) >= max_triplets:\n",
    "            break\n",
    "    return [InputExample(texts=[a, p, n]) for a, p, n in triplets if a and p and n]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990b53cb",
   "metadata": {},
   "source": [
    "### **Method to Fine Tune Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de54dc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated train_triplet_model\n",
    "def train_triplet_model(model_name, triplets, model_output,\n",
    "                        news_df_val, behaviors_df_val,\n",
    "                        epochs=3, warmup_steps=100, eval_k=10): # Added parameters\n",
    "    model = SentenceTransformer(model_name)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        model.to(\"cuda\")\n",
    "        print(\"✅ Model moved to GPU.\")\n",
    "    else:\n",
    "        print(\"⚠️ CUDA not available. Training on CPU.\")\n",
    "\n",
    "    train_loader = DataLoader(triplets, shuffle=True, batch_size=8)\n",
    "    loss_fn = losses.TripletLoss(model=model)\n",
    "\n",
    "    train_losses = []\n",
    "    val_precisions = []\n",
    "    val_recalls = []\n",
    "    val_ndcgs = []\n",
    "    val_maps = []\n",
    "    epoch_times = []\n",
    "\n",
    "    true_relevant_items_val = get_true_relevant_items_val(behaviors_df_val)\n",
    "    user_history_val = get_user_history_val(behaviors_df_val)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "    num_training_steps = len(train_loader) * epochs\n",
    "    warmup_steps_actual = int(num_training_steps * 0.1) if warmup_steps is None else warmup_steps\n",
    "\n",
    "    print(f\"\\nStarting fine-tuning for {model_name}...\")\n",
    "    global_step = 0\n",
    "    for epoch in range(epochs):\n",
    "        start_time = time.time()\n",
    "        model.train() # Set model to training mode\n",
    "        epoch_train_losses = []\n",
    "\n",
    "        for step, batch in tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch+1} Training\"):\n",
    "            features = model.tokenize(batch.texts)\n",
    "            reps = model(features)\n",
    "            loss = loss_fn(reps)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            epoch_train_losses.append(loss.item())\n",
    "            global_step += 1\n",
    "\n",
    "            if global_step < warmup_steps_actual:\n",
    "                for pg in optimizer.param_groups:\n",
    "                    pg['lr'] = (global_step / warmup_steps_actual) * 2e-5\n",
    "\n",
    "        avg_train_loss = np.mean(epoch_train_losses)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        print(f\"Epoch {epoch+1} - Avg Training Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "        # --- Validation Evaluation (Accuracy Curves) ---\n",
    "        model.eval() # Set model to evaluation mode\n",
    "        with torch.no_grad():\n",
    "            # Generate embeddings for validation news_df_val articles using current model state\n",
    "            val_news_texts_for_embedding = news_df_val['text'].tolist()\n",
    "            current_val_embeddings = model.encode(val_news_texts_for_embedding, show_progress_bar=False, convert_to_numpy=True)\n",
    "            # Create a temporary Faiss index for validation search\n",
    "            val_faiss_index = faiss.IndexFlatIP(current_val_embeddings.shape[1])\n",
    "            faiss.normalize_L2(current_val_embeddings)\n",
    "            val_faiss_index.add(current_val_embeddings)\n",
    "\n",
    "            # Simulate recommendations for validation users\n",
    "            val_recommended_items = simulate_recommendations_for_validation(\n",
    "                user_history_val,\n",
    "                model, # Pass the current model instance\n",
    "                None, # Tokenizer not needed for SentenceTransformer\n",
    "                val_faiss_index,\n",
    "                news_df_val, # Pass news_df_val for text lookup\n",
    "                k=eval_k\n",
    "            )\n",
    "\n",
    "            # Calculate metrics\n",
    "            metrics = calculate_precision_recall_ndcg_map(\n",
    "                val_recommended_items,\n",
    "                true_relevant_items_val,\n",
    "                k=eval_k\n",
    "            )\n",
    "            val_precisions.append(metrics[f\"Precision@{eval_k}\"])\n",
    "            val_recalls.append(metrics[f\"Recall@{eval_k}\"])\n",
    "            val_ndcgs.append(metrics[f\"NDCG@{eval_k}\"])\n",
    "            val_maps.append(metrics[f\"MAP@{eval_k}\"])\n",
    "            print(f\"Epoch {epoch+1} - Val Precision@{eval_k}: {metrics[f'Precision@{eval_k}']:.4f}, \"\n",
    "                  f\"Val Recall@{eval_k}: {metrics[f'Recall@{eval_k}']:.4f}, \"\n",
    "                  f\"Val NDCG@{eval_k}: {metrics[f'NDCG@{eval_k}']:.4f}, \"\n",
    "                  f\"Val MAP@{eval_k}: {metrics[f'MAP@{eval_k}']:.4f}\")\n",
    "\n",
    "        end_time_epoch = time.time()\n",
    "        epoch_times.append(end_time_epoch - start_time)\n",
    "        print(f\"Epoch {epoch+1} Time: {epoch_times[-1]:.2f} seconds\")\n",
    "\n",
    "    model.save(model_output)\n",
    "    print(f\"✅ Model Saved to '{model_output}'\")\n",
    "\n",
    "    return {\n",
    "        \"train_losses\": train_losses,\n",
    "        \"val_precisions\": val_precisions,\n",
    "        \"val_recalls\": val_recalls,\n",
    "        \"val_ndcgs\": val_ndcgs,\n",
    "        \"val_maps\": val_maps,\n",
    "        \"epoch_times\": epoch_times\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fac2f24",
   "metadata": {},
   "source": [
    "### **Models to Fine Tune - MiniLM and BERT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549efc74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model moved to GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     "
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1857' max='1857' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1857/1857 03:47, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>4.057800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>3.385900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>3.286500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model Saved to 'fine_tuned_minilm'\n",
      "✅ Model moved to GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     "
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1857' max='1857' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1857/1857 18:41, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.692800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.026000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.015600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model Saved to 'fine_tuned_bert'\n"
     ]
    }
   ],
   "source": [
    "# Updated Models to Fine Tune section\n",
    "news_dict_actual_train = dict(zip(news_df_actual_train['news_id'], news_df_actual_train['text']))\n",
    "\n",
    "triplets_initial = create_triplets(behaviors_df_actual_train, news_dict_actual_train)\n",
    "\n",
    "# Capture training results for MiniLM\n",
    "minilm_initial_results = train_triplet_model(\n",
    "    \"all-MiniLM-L6-v2\",\n",
    "    triplets_initial,\n",
    "    \"fine_tuned_minilm\",\n",
    "    news_df_val, # Pass validation data\n",
    "    behaviors_df_val, # Pass validation data\n",
    "    epochs=3 # Using 3 epochs as discussed\n",
    ")\n",
    "\n",
    "# Capture training results for BERT\n",
    "bert_initial_results = train_triplet_model(\n",
    "    \"bert-base-nli-mean-tokens\",\n",
    "    triplets_initial,\n",
    "    \"fine_tuned_bert\",\n",
    "    news_df_val, # Pass validation data\n",
    "    behaviors_df_val, # Pass validation data\n",
    "    epochs=3 # Using 3 epochs as discussed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7a4138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting initial fine-tuning results for MiniLM\n",
    "plot_training_metrics(minilm_initial_results, \"MiniLM Initial Fine-tuning\")\n",
    "\n",
    "# Plotting initial fine-tuning results for BERT\n",
    "plot_training_metrics(bert_initial_results, \"BERT Initial Fine-tuning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c1bdee",
   "metadata": {},
   "source": [
    "### **Fine Tuning Distilled Deepseek**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73ce141-a645-464b-8bc9-735ea3794671",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "Map: 100%|██████████| 1014/1014 [00:00<00:00, 2801.07 examples/s]\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "D:\\Anaconda\\envs\\ARS-Project\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='253' max='253' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [253/253 1:01:56, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>4.800400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>4.708700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>4.537200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>4.385500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>4.139300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>4.052500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>3.917900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>3.811100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>3.649400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.533600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>3.347500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>3.336600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>3.226600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>3.139600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>2.919400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>3.053900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>2.929800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>2.880900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>2.786200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.669700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>2.779200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>2.645200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>2.466600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>2.686200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>2.616700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\ARS-Project\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "D:\\Anaconda\\envs\\ARS-Project\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "D:\\Anaconda\\envs\\ARS-Project\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "D:\\Anaconda\\envs\\ARS-Project\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "D:\\Anaconda\\envs\\ARS-Project\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model fine-tuned and saved to './deepseek_peft_triplet'\n"
     ]
    }
   ],
   "source": [
    "from transformers import BitsAndBytesConfig, TrainerCallback # Ensure imported globally\n",
    "\n",
    "# Load DeepSeek tokenizer and model (8-bit)\n",
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "# Define the quantization config explicitly (using 4-bit as it's more memory efficient and common)\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, # Changed from load_in_8bit=True for better memory\n",
    "    bnb_4bit_quant_type=\"nf4\", # Recommended 4-bit quantization type\n",
    "    bnb_4bit_compute_dtype=torch.float16, # Recommended compute dtype for 4-bit\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=quantization_config, # Pass the config object here\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Prepare model for training\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Load and preprocess your dataset (triplet logic converted into prompt-style CLM)\n",
    "# IMPORTANT: Use news_df_actual_train\n",
    "news_dict_actual_train = dict(zip(news_df_actual_train['news_id'], news_df_actual_train['text']))\n",
    "\n",
    "# Format triplets as CLM-style instruction prompts\n",
    "def make_prompt(anchor, positive, negative):\n",
    "    # Corrected \\\\n to \\n for proper newlines\n",
    "    return f\"Anchor: {anchor}\\nPositive: {positive}\\nNegative: {negative}\\nWhich is more similar to the Anchor?\\nAnswer: Positive.\"\n",
    "\n",
    "triplet_prompts = []\n",
    "for _, row in behaviors_df_actual_train.iterrows(): # Use actual train behaviors_df\n",
    "    if pd.isna(row['impressions']):\n",
    "        continue\n",
    "    impressions = row['impressions'].split()\n",
    "    positives = [i.split('-')[0] for i in impressions if i.endswith('1')]\n",
    "    negatives = [i.split('-')[0] for i in impressions if i.endswith('0')]\n",
    "    if len(positives) < 2 or not negatives:\n",
    "        continue\n",
    "    for i in range(len(positives)):\n",
    "        anchor_id = positives[i]\n",
    "        pos_id = positives[(i + 1) % len(positives)]\n",
    "        for neg_id in negatives:\n",
    "            a, p, n = news_dict_actual_train.get(anchor_id), news_dict_actual_train.get(pos_id), news_dict_actual_train.get(neg_id) # Use actual train news_dict\n",
    "            if a and p and n:\n",
    "                triplet_prompts.append({\"text\": make_prompt(a, p, n)})\n",
    "    # Removed or significantly increased this limit for better training\n",
    "    if len(triplet_prompts) >= 20000: # Example: increased limit significantly\n",
    "        break\n",
    "\n",
    "# Convert prompts to HuggingFace Dataset\n",
    "dataset = Dataset.from_list(triplet_prompts)\n",
    "dataset = dataset.map(lambda x: tokenizer(x[\"text\"], truncation=True, padding=\"max_length\", max_length=512), batched=True)\n",
    "dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./deepseek_peft_triplet\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=3, # Using 3 epochs as discussed\n",
    "    logging_steps=10,\n",
    "    save_steps=50,\n",
    "    fp16=True,\n",
    "    save_total_limit=1,\n",
    "    report_to=\"none\",\n",
    "    disable_tqdm=False,\n",
    "    load_best_model_at_end=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    ")\n",
    "\n",
    "# Attach the custom callback for DeepSeek\n",
    "# Pass news_df_val and behaviors_df_val\n",
    "deepseek_initial_callback = CustomValidationCallback(news_df_val, behaviors_df_val, eval_k=10)\n",
    "trainer.add_callback(deepseek_initial_callback)\n",
    "\n",
    "# Train\n",
    "trainer.train()\n",
    "\n",
    "# Save\n",
    "model.save_pretrained(\"./deepseek_peft_triplet\")\n",
    "tokenizer.save_pretrained(\"./deepseek_peft_triplet\")\n",
    "\n",
    "print(\"✅ Model fine-tuned and saved to './deepseek_peft_triplet'\")\n",
    "\n",
    "# Extract results from the callback after training\n",
    "deepseek_initial_results = {\n",
    "    \"train_losses\": deepseek_initial_callback.train_losses_per_epoch,\n",
    "    \"val_precisions\": deepseek_initial_callback.val_precisions,\n",
    "    \"val_recalls\": deepseek_initial_callback.val_recalls,\n",
    "    \"val_ndcgs\": deepseek_initial_callback.val_ndcgs,\n",
    "    \"val_maps\": deepseek_initial_callback.val_maps,\n",
    "    \"epoch_times\": deepseek_initial_callback.epoch_times # This is validation time\n",
    "}\n",
    "\n",
    "# Plotting initial fine-tuning results for DeepSeek\n",
    "plot_training_metrics(deepseek_initial_results, \"DeepSeek Initial Fine-tuning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3d7a1d",
   "metadata": {},
   "source": [
    "## **Fine Tuned LLM - Based Embeddings Generation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515a242a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated generate_article_embeddings function\n",
    "def generate_article_embeddings(news_texts, model_path, model_type):\n",
    "    \"\"\"\n",
    "    Generates embeddings for news articles using a specified LLM.\n",
    "\n",
    "    Args:\n",
    "        news_texts (list): A list of strings, where each string is the text of a news article.\n",
    "        model_path (str): The path to the fine-tuned model.\n",
    "        model_type (str): The type of model to load (\"sentence_transformer\" or \"deepseek\").\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - np.ndarray: A NumPy array of embeddings.\n",
    "            - int: The dimensionality of the embeddings.\n",
    "    \"\"\"\n",
    "    print(f\"\\nGenerating embeddings using {model_type} model from '{model_path}'...\")\n",
    "    if model_type == \"sentence_transformer\":\n",
    "        model = SentenceTransformer(model_path)\n",
    "        if torch.cuda.is_available():\n",
    "            model.to(\"cuda\")\n",
    "        embeddings = model.encode(news_texts, show_progress_bar=True, convert_to_numpy=True)\n",
    "        embedding_dim = embeddings.shape[1]\n",
    "        print(f\"✅ Embeddings generated. Shape: {embeddings.shape}\")\n",
    "        return embeddings, embedding_dim\n",
    "\n",
    "    elif model_type == \"deepseek\":\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "        \n",
    "        # Using BitsAndBytesConfig explicitly as recommended\n",
    "        quantization_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    "        )\n",
    "        \n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_path,\n",
    "            quantization_config=quantization_config, # Pass the config object\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        if torch.cuda.is_available():\n",
    "            model.to(\"cuda\")\n",
    "        model.eval()\n",
    "\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "            model.config.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "        all_embeddings = []\n",
    "        batch_size = 8\n",
    "\n",
    "        for i in tqdm(range(0, len(news_texts), batch_size), desc=\"Generating DeepSeek Embeddings\"):\n",
    "            batch_texts = news_texts[i:i + batch_size]\n",
    "            inputs = tokenizer(batch_texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(model.device)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs, output_hidden_states=True)\n",
    "            # Corrected typo here: outputs.hidden_hidden_states -> outputs.hidden_states\n",
    "            last_hidden_states = outputs.hidden_states[-1]\n",
    "            attention_mask = inputs['attention_mask'].unsqueeze(-1).expand(last_hidden_states.size()).float()\n",
    "            masked_embeddings = last_hidden_states * attention_mask\n",
    "            sum_embeddings = torch.sum(masked_embeddings, 1)\n",
    "            sum_mask = torch.sum(attention_mask, 1)\n",
    "            mean_pooled_embeddings = sum_embeddings / torch.clamp(sum_mask, min=1e-9)\n",
    "            all_embeddings.extend(mean_pooled_embeddings.cpu().numpy())\n",
    "\n",
    "        embeddings = np.array(all_embeddings)\n",
    "        embedding_dim = embeddings.shape[1]\n",
    "        print(f\"✅ Embeddings generated. Shape: {embeddings.shape}\")\n",
    "        return embeddings, embedding_dim\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model_type: {model_type}. Choose 'sentence_transformer' or 'deepseek'.\")\n",
    "\n",
    "\n",
    "def build_annoy_index(embeddings, output_path_prefix):\n",
    "    \"\"\"\n",
    "    Builds and saves an Annoy index from given embeddings.\n",
    "\n",
    "    Args:\n",
    "        embeddings (np.ndarray): A NumPy array of embeddings.\n",
    "        output_path_prefix (str): A prefix for saving the Annoy index file and associated parquet.\n",
    "    Returns:\n",
    "        int: The embedding dimension used.\n",
    "    \"\"\"\n",
    "    embedding_dim = embeddings.shape[1]\n",
    "    print(f\"\\nBuilding Annoy index with {embedding_dim} dimensions for '{output_path_prefix}'...\")\n",
    "    annoy_index = AnnoyIndex(embedding_dim, 'angular')\n",
    "    for i, emb in tqdm(enumerate(embeddings), total=len(embeddings), desc=\"Adding items to Annoy index\"):\n",
    "        annoy_index.add_item(i, emb)\n",
    "    n_trees = 50\n",
    "    annoy_index.build(n_trees)\n",
    "    annoy_index_file = f'{output_path_prefix}_articles.ann'\n",
    "    annoy_index.save(annoy_index_file)\n",
    "    print(f\"✅ Annoy index built and saved to '{annoy_index_file}'\")\n",
    "    return embedding_dim\n",
    "\n",
    "\n",
    "def build_faiss_index_modular(embeddings, output_path_prefix):\n",
    "    \"\"\"\n",
    "    Builds and saves a Faiss index from given embeddings.\n",
    "\n",
    "    Args:\n",
    "        embeddings (np.ndarray): A NumPy array of embeddings.\n",
    "        output_path_prefix (str): A prefix for saving the Faiss index file.\n",
    "    \"\"\"\n",
    "    faiss.normalize_L2(embeddings)\n",
    "    embedding_dim = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatIP(embedding_dim)\n",
    "    index.add(embeddings)\n",
    "    faiss_index_file = f'{output_path_prefix}_articles.faiss'\n",
    "    faiss.write_index(index, faiss_index_file)\n",
    "    print(f\"✅ Faiss index built and saved to '{faiss_index_file}'\")\n",
    "\n",
    "# --- News texts prepared from Step 2, now from the training set ---\n",
    "# IMPORTANT: This variable should correctly point to news_df_train\n",
    "news_texts_for_embedding = news_df_train['text'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5a3fdd",
   "metadata": {},
   "source": [
    "### **MiniLM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2fcedc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Processing for MiniLM Model ---\n",
      "\n",
      "\n",
      "Generating embeddings using sentence_transformer model from 'fine_tuned_minilm'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1267/1267 [07:13<00:00,  2.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Embeddings generated. Shape: (40535, 384)\n",
      "\n",
      "Building Annoy index with 384 dimensions for 'fine_tuned_minilm'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding items to Annoy index: 100%|██████████| 40535/40535 [00:00<00:00, 41858.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Annoy index built and saved to 'fine_tuned_minilm_articles.ann'\n",
      "✅ News DataFrame with fine_tuned_minilm embeddings and Annoy IDs saved to 'fine_tuned_minilm_articles_with_embeddings_train.parquet'\n",
      "✅ Faiss index built and saved to 'fine_tuned_minilm_articles.faiss'\n",
      "\n",
      "--- MiniLM Model Processing Completed ---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Processing for MiniLM Model ---\\n\")\n",
    "\n",
    "# Define MiniLM model paths\n",
    "# Ensure 'fine_tuned_minilm' is the correct path where your first-stage fine-tuned model is saved\n",
    "minilm_model_path = \"fine_tuned_minilm\"\n",
    "minilm_output_prefix = minilm_model_path.replace(\"./\", \"\").replace(\"/\", \"_\")\n",
    "\n",
    "# Step 4: Generate Article Embeddings for MiniLM using the training data\n",
    "minilm_embeddings, minilm_embedding_dim = generate_article_embeddings(\n",
    "    news_texts_for_embedding, # This now correctly refers to news_df_train['text'].tolist()\n",
    "    minilm_model_path,\n",
    "    \"sentence_transformer\"\n",
    ")\n",
    "# Store embeddings in DataFrame for this model (optional, for persistence/inspection)\n",
    "# IMPORTANT: Store with news_df_train\n",
    "news_df_train[f'{minilm_output_prefix}_embedding'] = list(minilm_embeddings)\n",
    "\n",
    "\n",
    "# Step 5: Build LSH Index (Annoy) for MiniLM using the training data embeddings\n",
    "build_annoy_index(minilm_embeddings, minilm_output_prefix)\n",
    "# Save updated news_df_train with MiniLM embeddings and Annoy IDs\n",
    "# IMPORTANT: Use news_df_train\n",
    "news_df_train['annoy_id'] = news_df_train.index # Ensure index is stored for Annoy lookup\n",
    "news_df_train.to_parquet(f'{minilm_output_prefix}_articles_with_embeddings_train.parquet', engine='pyarrow', index=False)\n",
    "print(f\"✅ News DataFrame with {minilm_output_prefix} embeddings and Annoy IDs saved to '{minilm_output_prefix}_articles_with_embeddings_train.parquet'\")\n",
    "\n",
    "\n",
    "# Step 6: Build Faiss Index for MiniLM using the training data embeddings\n",
    "build_faiss_index_modular(minilm_embeddings, minilm_output_prefix)\n",
    "\n",
    "print(\"\\n--- MiniLM Model Processing Completed ---\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a720fc",
   "metadata": {},
   "source": [
    "### **BERT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a7713a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Processing for BERT Model ---\n",
      "\n",
      "\n",
      "Generating embeddings using sentence_transformer model from 'fine_tuned_bert'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1267/1267 [3:06:50<00:00,  8.85s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Embeddings generated. Shape: (40535, 768)\n",
      "\n",
      "Building Annoy index with 768 dimensions for 'fine_tuned_bert'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding items to Annoy index: 100%|██████████| 40535/40535 [00:01<00:00, 20590.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Annoy index built and saved to 'fine_tuned_bert_articles.ann'\n",
      "✅ News DataFrame with fine_tuned_bert embeddings and Annoy IDs saved to 'fine_tuned_bert_articles_with_embeddings_train.parquet'\n",
      "✅ Faiss index built and saved to 'fine_tuned_bert_articles.faiss'\n",
      "\n",
      "--- BERT Model Processing Completed ---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Processing for BERT Model ---\\n\")\n",
    "\n",
    "# Define BERT model paths\n",
    "# Ensure 'fine_tuned_bert' is the correct path where your first-stage fine-tuned model is saved\n",
    "bert_model_path = \"fine_tuned_bert\"\n",
    "bert_output_prefix = bert_model_path.replace(\"./\", \"\").replace(\"/\", \"_\")\n",
    "\n",
    "# Step 4: Generate Article Embeddings for BERT using the training data\n",
    "bert_embeddings, bert_embedding_dim = generate_article_embeddings(\n",
    "    news_texts_for_embedding, # This correctly refers to news_df_train['text'].tolist()\n",
    "    bert_model_path,\n",
    "    \"sentence_transformer\"\n",
    ")\n",
    "# Store embeddings in DataFrame for this model (optional, for persistence/inspection)\n",
    "# IMPORTANT: Store with news_df_train\n",
    "news_df_train[f'{bert_output_prefix}_embedding'] = list(bert_embeddings)\n",
    "\n",
    "\n",
    "# Step 5: Build LSH Index (Annoy) for BERT using the training data embeddings\n",
    "build_annoy_index(bert_embeddings, bert_output_prefix)\n",
    "# Save updated news_df_train with BERT embeddings and Annoy IDs\n",
    "# IMPORTANT: Use news_df_train\n",
    "news_df_train.to_parquet(f'{bert_output_prefix}_articles_with_embeddings_train.parquet', engine='pyarrow', index=False)\n",
    "print(f\"✅ News DataFrame with {bert_output_prefix} embeddings and Annoy IDs saved to '{bert_output_prefix}_articles_with_embeddings_train.parquet'\")\n",
    "\n",
    "\n",
    "# Step 6: Build Faiss Index for BERT using the training data embeddings\n",
    "build_faiss_index_modular(bert_embeddings, bert_output_prefix)\n",
    "\n",
    "print(\"\\n--- BERT Model Processing Completed ---\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02bc5c0",
   "metadata": {},
   "source": [
    "### **Deepseek**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d89b8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Processing for DeepSeek Model ---\n",
      "\n",
      "\n",
      "Generating embeddings using deepseek model from './deepseek_peft_triplet'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "Generating DeepSeek Embeddings: 100%|██████████| 5067/5067 [58:42<00:00,  1.44it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Embeddings generated. Shape: (40535, 1536)\n",
      "\n",
      "Building Annoy index with 1536 dimensions for 'deepseek_peft_triplet'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding items to Annoy index: 100%|██████████| 40535/40535 [00:04<00:00, 9829.42it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Annoy index built and saved to 'deepseek_peft_triplet_articles.ann'\n",
      "✅ News DataFrame with deepseek_peft_triplet embeddings and Annoy IDs saved to 'deepseek_peft_triplet_articles_with_embeddings_train.parquet'\n",
      "✅ Faiss index built and saved to 'deepseek_peft_triplet_articles.faiss'\n",
      "\n",
      "--- DeepSeek Model Processing Completed ---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Processing for DeepSeek Model ---\\n\")\n",
    "\n",
    "# Define DeepSeek model paths\n",
    "# Ensure './deepseek_peft_triplet' is the correct path where your first-stage fine-tuned model is saved\n",
    "deepseek_model_path = \"./deepseek_peft_triplet\"\n",
    "deepseek_output_prefix = deepseek_model_path.replace(\"./\", \"\").replace(\"/\", \"_\") # Clean up path for filename\n",
    "\n",
    "# Step 4: Generate Article Embeddings for DeepSeek using the training data\n",
    "deepseek_embeddings, deepseek_embedding_dim = generate_article_embeddings(\n",
    "    news_texts_for_embedding, # This correctly refers to news_df_train['text'].tolist()\n",
    "    deepseek_model_path,\n",
    "    \"deepseek\"\n",
    ")\n",
    "# Store embeddings in DataFrame for this model (optional, for persistence/inspection)\n",
    "# IMPORTANT: Store with news_df_train\n",
    "news_df_train[f'{deepseek_output_prefix}_embedding'] = list(deepseek_embeddings)\n",
    "\n",
    "\n",
    "# Step 5: Build LSH Index (Annoy) for DeepSeek using the training data embeddings\n",
    "build_annoy_index(deepseek_embeddings, deepseek_output_prefix)\n",
    "# Save updated news_df_train with DeepSeek embeddings and Annoy IDs\n",
    "# IMPORTANT: Use news_df_train\n",
    "news_df_train.to_parquet(f'{deepseek_output_prefix}_articles_with_embeddings_train.parquet', engine='pyarrow', index=False)\n",
    "print(f\"✅ News DataFrame with {deepseek_output_prefix} embeddings and Annoy IDs saved to '{deepseek_output_prefix}_articles_with_embeddings_train.parquet'\")\n",
    "\n",
    "\n",
    "# Step 6: Build Faiss Index for DeepSeek using the training data embeddings\n",
    "build_faiss_index_modular(deepseek_embeddings, deepseek_output_prefix)\n",
    "\n",
    "print(\"\\n--- DeepSeek Model Processing Completed ---\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133218ca",
   "metadata": {},
   "source": [
    "## **2.3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5244ecfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a helper function to load a Faiss index, assuming it was saved.\n",
    "def load_faiss_index(index_path):\n",
    "    print(f\"Loading Faiss index from '{index_path}'...\")\n",
    "    index = faiss.read_index(index_path)\n",
    "    print(f\"✅ Faiss index loaded from '{index_path}'\")\n",
    "    return index\n",
    "\n",
    "# Helper to load fine-tuned SentenceTransformer model\n",
    "def load_sentence_transformer_model(model_path):\n",
    "    model = SentenceTransformer(model_path)\n",
    "    if torch.cuda.is_available():\n",
    "        model.to(\"cuda\")\n",
    "    return model\n",
    "\n",
    "# Helper to load fine-tuned DeepSeek model for embedding generation\n",
    "def load_deepseek_model_for_embedding(model_path):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "    # Using BitsAndBytesConfig explicitly as recommended\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True, # Changed from load_in_8bit=True\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "    )\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_path,\n",
    "        quantization_config=quantization_config, # Pass the config object\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    if torch.cuda.is_available():\n",
    "        model.to(\"cuda\")\n",
    "    model.eval()\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        model.config.pad_token_id = tokenizer.eos_token_id\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc068d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated generate_hard_triplets\n",
    "def generate_hard_triplets(\n",
    "    faiss_index,\n",
    "    embedding_model, # The fine-tuned S-Transformer or DeepSeek model instance\n",
    "    tokenizer, # Only for DeepSeek\n",
    "    news_texts_dict, # Dictionary mapping news_id to text (should be derived from news_df_train)\n",
    "    behaviors_df_train, # IMPORTANT: Now explicitly taking the training behaviors_df\n",
    "    num_hard_negatives_per_anchor=1,\n",
    "    max_hard_triplets=5000 # Limit the number of hard triplets\n",
    "):\n",
    "    print(f\"\\nGenerating hard triplets (up to {max_hard_triplets})...\")\n",
    "    hard_triplets = []\n",
    "    processed_users = set()\n",
    "\n",
    "    # Create a mapping from news_id to its index in the embedding array\n",
    "    # This must be based on the news_df_train used to build the Faiss index\n",
    "    # Assuming news_df_train has news_id and that its current index directly maps to embedding array\n",
    "    news_id_to_idx = {news_id: idx for idx, news_id in enumerate(news_df_train['news_id'].tolist())}\n",
    "    idx_to_news_id = {idx: news_id for news_id, idx in news_id_to_idx.items()}\n",
    "\n",
    "\n",
    "    for _, row in tqdm(behaviors_df_train.iterrows(), total=len(behaviors_df_train), desc=\"Mining Hard Negatives\"): # Use behaviors_df_train\n",
    "        user_id = row[\"user_id\"]\n",
    "        if user_id in processed_users:\n",
    "            continue\n",
    "        processed_users.add(user_id)\n",
    "\n",
    "        if pd.isna(row[\"impressions\"]): continue\n",
    "        impressions = row[\"impressions\"].split()\n",
    "        pos_news_ids = [i.split('-')[0] for i in impressions if i.endswith('1')]\\\n",
    "        neg_news_ids = [i.split('-')[0] for i in impressions if i.endswith('0')]\n",
    "\n",
    "        if len(pos_news_ids) < 1 or not neg_news_ids: continue\n",
    "\n",
    "        # Use only articles that exist in our news_texts_dict (from news_df_train)\n",
    "        valid_pos_news_ids = [nid for nid in pos_news_ids if nid in news_id_to_idx and news_texts_dict.get(nid)]\n",
    "        valid_neg_news_ids = [nid for nid in neg_news_ids if nid in news_id_to_idx and news_texts_dict.get(nid)]\n",
    "\n",
    "        if len(valid_pos_news_ids) < 1 or not valid_neg_news_ids: continue\n",
    "\n",
    "        for anchor_id in valid_pos_news_ids:\n",
    "            # Get anchor text and its index\n",
    "            anchor_text = news_texts_dict.get(anchor_id)\n",
    "            anchor_idx = news_id_to_idx.get(anchor_id)\n",
    "            if not anchor_text or anchor_idx is None: continue\n",
    "\n",
    "            # Generate embedding for the anchor\n",
    "            if isinstance(embedding_model, SentenceTransformer):\n",
    "                anchor_embedding = embedding_model.encode([anchor_text], convert_to_numpy=True)\n",
    "            elif isinstance(embedding_model, AutoModelForCausalLM): # For DeepSeek\n",
    "                inputs = tokenizer(anchor_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(embedding_model.device)\n",
    "                with torch.no_grad():\n",
    "                    outputs = embedding_model(**inputs, output_hidden_states=True)\n",
    "                # Corrected typo here: outputs.hidden_hidden_states -> outputs.hidden_states\n",
    "                last_hidden_states = outputs.hidden_states[-1]\n",
    "                attention_mask = inputs['attention_mask'].unsqueeze(-1).expand(last_hidden_states.size()).float()\n",
    "                masked_embeddings = last_hidden_states * attention_mask\n",
    "                sum_embeddings = torch.sum(masked_embeddings, 1)\n",
    "                sum_mask = torch.sum(attention_mask, 1)\n",
    "                anchor_embedding = (sum_embeddings / torch.clamp(sum_mask, min=1e-9)).cpu().numpy()\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            # Normalize anchor embedding for IP search\n",
    "            faiss.normalize_L2(anchor_embedding)\n",
    "\n",
    "            # Search for nearest neighbors (both positive and negative) in the index\n",
    "            D, I = faiss_index.search(anchor_embedding, k=min(faiss_index.ntotal, 200))\n",
    "\n",
    "            # Filter for hard negatives: these are recommended but user didn't click\n",
    "            hard_neg_candidates = []\n",
    "            for j, neighbor_idx in enumerate(I[0]):\n",
    "                neighbor_id = idx_to_news_id.get(neighbor_idx)\n",
    "                if neighbor_id and neighbor_id in valid_neg_news_ids:\n",
    "                    hard_neg_candidates.append(news_texts_dict.get(neighbor_id))\n",
    "\n",
    "            # Create triplets with hard negatives\n",
    "            if hard_neg_candidates:\n",
    "                other_pos_ids = [nid for nid in valid_pos_news_ids if nid != anchor_id]\n",
    "                if not other_pos_ids: continue\n",
    "\n",
    "                positive_id = random.choice(other_pos_ids)\n",
    "                positive_text = news_texts_dict.get(positive_id)\n",
    "                if not positive_text: continue\n",
    "\n",
    "                selected_hard_negatives = random.sample(hard_neg_candidates, min(len(hard_neg_candidates), num_hard_negatives_per_anchor))\n",
    "\n",
    "                for hard_neg_text in selected_hard_negatives:\n",
    "                    hard_triplets.append(InputExample(texts=[anchor_text, positive_text, hard_neg_text]))\n",
    "                    if len(hard_triplets) >= max_hard_triplets:\n",
    "                        print(f\"✅ Reached maximum hard triplets: {max_hard_triplets}\")\n",
    "                        return hard_triplets\n",
    "\n",
    "    print(f\"✅ Generated {len(hard_triplets)} hard triplets.\")\n",
    "    return hard_triplets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1f267c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated train_second_stage_triplet_model\n",
    "def train_second_stage_triplet_model(model_instance, hard_triplets, model_output_path_suffix,\n",
    "                                     news_df_val, behaviors_df_val, epochs=1, warmup_steps=50, eval_k=10):\n",
    "    \"\"\"\n",
    "    Continues fine-tuning a SentenceTransformer model using hard triplets.\n",
    "    Includes logging of train/val metrics and epoch times.\n",
    "    \"\"\"\n",
    "    model_output = f\"{model_instance.cache_folder.split('/')[-1]}_{model_output_path_suffix}\"\n",
    "    if torch.cuda.is_available():\n",
    "        model_instance.to(\"cuda\")\n",
    "        print(\"✅ Model moved to GPU for second stage training.\")\n",
    "    else:\n",
    "        print(\"⚠️ CUDA not available. Second stage training on CPU.\")\n",
    "\n",
    "    train_loader = DataLoader(hard_triplets, shuffle=True, batch_size=8)\n",
    "    loss_fn = losses.TripletLoss(model=model_instance)\n",
    "    optimizer = torch.optim.AdamW(model_instance.parameters(), lr=1e-5) # Smaller LR for second stage\n",
    "\n",
    "    train_losses = []\n",
    "    val_precisions = []\n",
    "    val_recalls = []\n",
    "    val_ndcgs = []\n",
    "    val_maps = []\n",
    "    epoch_times = []\n",
    "\n",
    "    true_relevant_items_val = get_true_relevant_items_val(behaviors_df_val)\n",
    "    user_history_val = get_user_history_val(behaviors_df_val)\n",
    "\n",
    "    print(f\"\\nStarting second-stage fine-tuning for {model_output}...\")\n",
    "    global_step = 0\n",
    "    for epoch in range(epochs):\n",
    "        start_time = time.time()\n",
    "        model_instance.train()\n",
    "        epoch_train_losses = []\n",
    "\n",
    "        for step, batch in tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Second Stage Epoch {epoch+1} Training\"):\n",
    "            features = model_instance.tokenize(batch.texts)\n",
    "            reps = model_instance(features)\n",
    "            loss = loss_fn(reps)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            epoch_train_losses.append(loss.item())\n",
    "            global_step += 1\n",
    "\n",
    "            if global_step < warmup_steps:\n",
    "                for pg in optimizer.param_groups:\n",
    "                    pg['lr'] = (global_step / warmup_steps) * 1e-5\n",
    "\n",
    "        avg_train_loss = np.mean(epoch_train_losses)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        print(f\"Second Stage Epoch {epoch+1} - Avg Training Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "        # --- Validation Evaluation ---\n",
    "        model_instance.eval()\n",
    "        with torch.no_grad():\n",
    "            val_news_texts_for_embedding = news_df_val['text'].tolist()\n",
    "            current_val_embeddings = model_instance.encode(val_news_texts_for_embedding, show_progress_bar=False, convert_to_numpy=True)\n",
    "            val_faiss_index = faiss.IndexFlatIP(current_val_embeddings.shape[1])\n",
    "            faiss.normalize_L2(current_val_embeddings)\n",
    "            val_faiss_index.add(current_val_embeddings)\n",
    "            \n",
    "            val_recommended_items = simulate_recommendations_for_validation(\n",
    "                user_history_val,\n",
    "                model_instance,\n",
    "                None, # Tokenizer not needed for SentenceTransformer\n",
    "                val_faiss_index,\n",
    "                news_df_val,\n",
    "                k=eval_k\n",
    "            )\n",
    "            metrics = calculate_precision_recall_ndcg_map(val_recommended_items, true_relevant_items_val, k=eval_k)\n",
    "            val_precisions.append(metrics[f\"Precision@{eval_k}\"])\n",
    "            val_recalls.append(metrics[f\"Recall@{eval_k}\"])\n",
    "            val_ndcgs.append(metrics[f\"NDCG@{eval_k}\"])\n",
    "            val_maps.append(metrics[f\"MAP@{eval_k}\"])\n",
    "            print(f\"Second Stage Epoch {epoch+1} - Val Precision@{eval_k}: {metrics[f'Precision@{eval_k}']:.4f}, \"\n",
    "                  f\"Val Recall@{eval_k}: {metrics[f'Recall@{eval_k}']:.4f}, \"\n",
    "                  f\"Val NDCG@{eval_k}: {metrics[f'NDCG@{eval_k}']:.4f}, \"\n",
    "                  f\"Val MAP@{eval_k}: {metrics[f'MAP@{eval_k}']:.4f}\")\n",
    "\n",
    "        end_time_epoch = time.time()\n",
    "        epoch_times.append(end_time_epoch - start_time)\n",
    "        print(f\"Second Stage Epoch {epoch+1} Time: {epoch_times[-1]:.2f} seconds\")\n",
    "\n",
    "    final_output_path = model_output # Assume model_output is the full path\n",
    "    model_instance.save(final_output_path)\n",
    "    print(f\"✅ Second-stage fine-tuned model saved to '{final_output_path}'\")\n",
    "    return {\n",
    "        \"train_losses\": train_losses,\n",
    "        \"val_precisions\": val_precisions,\n",
    "        \"val_recalls\": val_recalls,\n",
    "        \"val_ndcgs\": val_ndcgs,\n",
    "        \"val_maps\": val_maps,\n",
    "        \"epoch_times\": epoch_times\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b06eca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated train_second_stage_deepseek\n",
    "class CustomValidationCallback(TrainerCallback):\n",
    "    def __init__(self, news_df_val, behaviors_df_val, eval_k=10):\n",
    "        self.news_df_val = news_df_val\n",
    "        self.behaviors_df_val = behaviors_df_val\n",
    "        self.eval_k = eval_k\n",
    "        self.val_precisions = []\n",
    "        self.val_recalls = []\n",
    "        self.val_ndcgs = []\n",
    "        self.val_maps = []\n",
    "        self.epoch_times = [] # To store the time taken for the validation step\n",
    "        self.train_losses_per_epoch = []\n",
    "        self.true_relevant_items_val = get_true_relevant_items_val(behaviors_df_val)\n",
    "        self.user_history_val = get_user_history_val(behaviors_df_val)\n",
    "        self.news_id_to_idx_val = {news_id: idx for idx, news_id in enumerate(self.news_df_val['news_id'].tolist())}\n",
    "        self.idx_to_news_id_val = {idx: news_id for news_id, idx in self.news_id_to_idx_val.items()}\n",
    "\n",
    "\n",
    "    def on_epoch_end(self, args, state, control, **kwargs):\n",
    "        # Calculate training loss for the epoch\n",
    "        if state.log_history:\n",
    "            epoch_logs = [log for log in state.log_history if 'epoch' in log and round(log['epoch']) == round(state.epoch)]\n",
    "            if epoch_logs:\n",
    "                epoch_loss_values = [log['loss'] for log in epoch_logs if 'loss' in log]\n",
    "                if epoch_loss_values:\n",
    "                    self.train_losses_per_epoch.append(np.mean(epoch_loss_values))\n",
    "\n",
    "        start_time_val_epoch = time.time() # Start timing for this epoch's validation\n",
    "\n",
    "        self.trainer.model.eval() # Set model to eval mode\n",
    "\n",
    "        # Dynamically generate embeddings for news_df_val using the current model and tokenizer\n",
    "        val_news_texts_for_embedding_current = self.news_df_val['text'].tolist()\n",
    "        \n",
    "        current_val_embeddings_list = []\n",
    "        batch_size_val_inference = 8 # Batch size for validation inference\n",
    "        \n",
    "        # Access the model and tokenizer from the trainer\n",
    "        current_deepseek_model = self.trainer.model\n",
    "        current_deepseek_tokenizer = self.trainer.tokenizer\n",
    "\n",
    "        for i in tqdm(range(0, len(val_news_texts_for_embedding_current), batch_size_val_inference), desc=\"Generating Val Embeddings (DeepSeek)\"):\n",
    "            batch_texts = val_news_texts_for_embedding_current[i:i + batch_size_val_inference]\n",
    "            inputs = current_deepseek_tokenizer(batch_texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(current_deepseek_model.device)\n",
    "            with torch.no_grad():\n",
    "                outputs = current_deepseek_model(**inputs, output_hidden_states=True)\n",
    "            # Corrected typo: outputs.hidden_hidden_states -> outputs.hidden_states\n",
    "            last_hidden_states = outputs.hidden_states[-1]\n",
    "            attention_mask = inputs['attention_mask'].unsqueeze(-1).expand(last_hidden_states.size()).float()\n",
    "            masked_embeddings = last_hidden_states * attention_mask\n",
    "            sum_embeddings = torch.sum(masked_embeddings, 1)\n",
    "            sum_mask = torch.sum(attention_mask, 1)\n",
    "            mean_pooled_embeddings = sum_embeddings / torch.clamp(sum_mask, min=1e-9)\n",
    "            current_val_embeddings_list.extend(mean_pooled_embeddings.cpu().numpy())\n",
    "        current_val_embeddings_np = np.array(current_val_embeddings_list)\n",
    "\n",
    "\n",
    "        val_faiss_index = faiss.IndexFlatIP(current_val_embeddings_np.shape[1])\n",
    "        faiss.normalize_L2(current_val_embeddings_np)\n",
    "        val_faiss_index.add(current_val_embeddings_np)\n",
    "        \n",
    "        val_recommended_items = simulate_recommendations_for_validation(\n",
    "            self.user_history_val,\n",
    "            current_deepseek_model, # Pass the current model instance\n",
    "            current_deepseek_tokenizer, # Pass the tokenizer for DeepSeek\n",
    "            val_faiss_index,\n",
    "            self.news_df_val,\n",
    "            k=self.eval_k\n",
    "        )\n",
    "        \n",
    "        metrics = calculate_precision_recall_ndcg_map(\n",
    "            val_recommended_items,\n",
    "            self.true_relevant_items_val,\n",
    "            k=self.eval_k\n",
    "        )\n",
    "        self.val_precisions.append(metrics[f\"Precision@{self.eval_k}\"])\n",
    "        self.val_recalls.append(metrics[f\"Recall@{self.eval_k}\"])\n",
    "        self.val_ndcgs.append(metrics[f\"NDCG@{self.eval_k}\"])\n",
    "        self.val_maps.append(metrics[f\"MAP@{self.eval_k}\"])\n",
    "        \n",
    "        end_time_val_epoch = time.time()\n",
    "        self.epoch_times.append(end_time_val_epoch - start_time_val_epoch)\n",
    "\n",
    "        print(f\"Epoch {int(state.epoch)} - Val Precision@{self.eval_k}: {metrics[f'Precision@{self.eval_k}']:.4f}, \"\n",
    "              f\"Val Recall@{self.eval_k}: {metrics[f'Recall@{self.eval_k}']:.4f}, \"\n",
    "              f\"Val NDCG@{self.eval_k}: {metrics[f'NDCG@{self.eval_k}']:.4f}, \"\n",
    "              f\"Val MAP@{self.eval_k}: {metrics[f'MAP@{self.eval_k}']:.4f}, \"\n",
    "              f\"Val Time: {self.epoch_times[-1]:.2f}s\")\n",
    "        \n",
    "        control.should_save = True # Optional: force save checkpoint at epoch end\n",
    "\n",
    "    def on_train_end(self, args, state, control, **kwargs):\n",
    "        # This method is called once at the very end of training.\n",
    "        # epoch_times and train_losses_per_epoch are already populated by on_epoch_end\n",
    "        pass # No additional aggregation needed here if on_epoch_end is working as intended\n",
    "\n",
    "\n",
    "\n",
    "# Updated train_second_stage_deepseek\n",
    "def train_second_stage_deepseek(model_instance, tokenizer_instance, hard_triplets, model_output_path_suffix,\n",
    "                                news_df_val, behaviors_df_val, epochs=1, eval_k=10):\n",
    "    \"\"\"\n",
    "    Continues fine-tuning DeepSeek using hard triplets as prompts.\n",
    "    Includes logging of train/val metrics and epoch times via a custom callback.\n",
    "    \"\"\"\n",
    "    model_output_dir = f\"./deepseek_peft_triplet_{model_output_path_suffix}\"\n",
    "    print(f\"\\nStarting second-stage fine-tuning for DeepSeek to '{model_output_dir}'...\")\n",
    "\n",
    "    def make_prompt(anchor, positive, negative):\n",
    "        # Corrected \\n for proper newlines\n",
    "        return f\"Anchor: {anchor}\\nPositive: {positive}\\nNegative: {negative}\\nWhich is more similar to the Anchor?\\nAnswer: Positive.\"\n",
    "\n",
    "    triplet_prompts = []\n",
    "    for example in hard_triplets:\n",
    "        a, p, n = example.texts[0], example.texts[1], example.texts[2]\n",
    "        triplet_prompts.append({\"text\": make_prompt(a, p, n)})\n",
    "\n",
    "    dataset = Dataset.from_list(triplet_prompts)\n",
    "    dataset = dataset.map(lambda x: tokenizer_instance(x[\"text\"], truncation=True, padding=\"max_length\", max_length=512), batched=True)\n",
    "    dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=model_output_dir,\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=4,\n",
    "        num_train_epochs=epochs, # Use the epochs parameter\n",
    "        logging_steps=10,\n",
    "        save_steps=50,\n",
    "        fp16=True,\n",
    "        save_total_limit=1,\n",
    "        report_to=\"none\",\n",
    "        disable_tqdm=False,\n",
    "        load_best_model_at_end=False,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model_instance,\n",
    "        args=training_args,\n",
    "        train_dataset=dataset,\n",
    "        data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer_instance, mlm=False)\n",
    "    )\n",
    "\n",
    "    # Attach the custom callback for DeepSeek\n",
    "    callback_instance = CustomValidationCallback(news_df_val, behaviors_df_val, eval_k=eval_k)\n",
    "    trainer.add_callback(callback_instance)\n",
    "    \n",
    "    # Start training\n",
    "    trainer.train()\n",
    "\n",
    "    # Save final model\n",
    "    model_instance.save_pretrained(model_output_dir)\n",
    "    tokenizer_instance.save_pretrained(model_output_dir)\n",
    "    print(f\"✅ Second-stage fine-tuned DeepSeek model saved to '{model_output_dir}'\")\n",
    "\n",
    "    # Return results collected by the callback\n",
    "    return {\n",
    "        \"train_losses\": callback_instance.train_losses_per_epoch,\n",
    "        \"val_precisions\": callback_instance.val_precisions,\n",
    "        \"val_recalls\": callback_instance.val_recalls,\n",
    "        \"val_ndcgs\": callback_instance.val_ndcgs,\n",
    "        \"val_maps\": callback_instance.val_maps,\n",
    "        \"epoch_times\": callback_instance.epoch_times # This is validation time per epoch\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763bde3f-45ee-4e9d-a356-d61787f575c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting R2.3 Step 2 for MiniLM (Hard Negative Mining) ---\n",
      "Loading Faiss index from 'fine_tuned_minilm_articles.faiss'...\n",
      "✅ Faiss index loade|d from 'fine_tuned_minilm_articles.faiss'\n",
      "\n",
      "Generating hard triplets (up to 5000)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mining Hard Negatives:  58%|█████▊    | 72623/125634 [07:47<05:41, 155.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Reached maximum hard triplets: 5000\n",
      "✅ Model moved to GPU for second stage training.\n",
      "\n",
      "Starting second-stage fine-tuning for fine_tuned_minilm_second_stage...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     "
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='625' max='625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [625/625 00:55, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>4.901400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Second-stage fine-tuned model saved to 'fine_tuned_minilm_second_stage'\n",
      "\n",
      "Generating embeddings from second-stage fine-tuned MiniLM model: fine_tuned_minilm_second_stage\n",
      "\n",
      "Generating embeddings using sentence_transformer model from 'fine_tuned_minilm_second_stage'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1267/1267 [00:26<00:00, 47.77it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Embeddings generated. Shape: (40535, 384)\n",
      "\n",
      "Building Annoy index with 384 dimensions for 'fine_tuned_minilm_second_stage'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding items to Annoy index: 100%|██████████| 40535/40535 [00:00<00:00, 41595.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Annoy index built and saved to 'fine_tuned_minilm_second_stage_articles.ann'\n",
      "✅ Faiss index built and saved to 'fine_tuned_minilm_second_stage_articles.faiss'\n",
      "--- R2.3 Step 2 for MiniLM Completed ---\n"
     ]
    }
   ],
   "source": [
    "# Updated MiniLM Second Stage Call\n",
    "print(\"\\n--- Starting R2.3 Step 2 for MiniLM (Hard Negative Mining) ---\")\n",
    "\n",
    "minilm_model_for_hnm = load_sentence_transformer_model(\"fine_tuned_minilm\")\n",
    "minilm_faiss_index = load_faiss_index(\"fine_tuned_minilm_articles.faiss\")\n",
    "\n",
    "news_dict_for_hnm = dict(zip(news_df_actual_train['news_id'], news_df_actual_train['text'])) # Use actual train news_df\n",
    "\n",
    "minilm_hard_triplets = generate_hard_triplets(\n",
    "    faiss_index=minilm_faiss_index,\n",
    "    embedding_model=minilm_model_for_hnm,\n",
    "    tokenizer=None,\n",
    "    news_texts_dict=news_dict_for_hnm,\n",
    "    behaviors_df_train=behaviors_df_actual_train, # Use actual train behaviors_df\n",
    "    max_hard_triplets=5000\n",
    ")\n",
    "\n",
    "if minilm_hard_triplets:\n",
    "    minilm_second_stage_results = train_second_stage_triplet_model( # Capture results\n",
    "        model_instance=minilm_model_for_hnm,\n",
    "        hard_triplets=minilm_hard_triplets,\n",
    "        model_output_path_suffix=\"second_stage\",\n",
    "        news_df_val=news_df_val, # Pass validation data\n",
    "        behaviors_df_val=behaviors_df_val, # Pass validation data\n",
    "        epochs=1 # Using 1 epoch for second stage as a starting point\n",
    "    )\n",
    "\n",
    "    # Plotting second-stage results for MiniLM\n",
    "    plot_training_metrics(minilm_second_stage_results, \"MiniLM Second Stage Fine-tuning\")\n",
    "\n",
    "    # ... (rest of MiniLM second stage: generate embeddings and rebuild indices) ...\n",
    "    second_stage_minilm_embeddings, _ = generate_article_embeddings(\n",
    "        news_texts_for_embedding, # This uses news_df_train (which is news_df_actual_train + news_df_val)\n",
    "        minilm_second_stage_model_path,\n",
    "        \"sentence_transformer\"\n",
    "    )\n",
    "    # The `news_texts_for_embedding` here should be `news_df_actual_train['text'].tolist()`\n",
    "    # if you want to generate embeddings for only the \"actual_train\" set for the index.\n",
    "    # Otherwise, it's `news_df_train['text'].tolist()` if you want embeddings for the full initial training set.\n",
    "    # For indices, usually you want embeddings for ALL articles that could potentially be recommended, so\n",
    "    # news_df_train (which contains news_df_actual_train + news_df_val) is appropriate here.\n",
    "    # Re-check the initial news_texts_for_embedding definition to ensure it uses news_df_train.\n",
    "    # If the initial definition in `Fine Tuned LLM - Based Embeddings Generation` is `news_df_train['text'].tolist()`, it's correct.\n",
    "\n",
    "    # Re-build Annoy and Faiss indices with the *second-stage* embeddings\n",
    "    # Save these with a distinct prefix to indicate they are from the second stage\n",
    "    build_annoy_index(second_stage_minilm_embeddings, \"fine_tuned_minilm_second_stage\")\n",
    "    build_faiss_index_modular(second_stage_minilm_embeddings, \"fine_tuned_minilm_second_stage\")\n",
    "    print(\"--- R2.3 Step 2 for MiniLM Completed ---\")\n",
    "else:\n",
    "    print(\"No hard triplets found for MiniLM. Skipping second-stage fine-tuning.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ee4dd0-feb6-4f9d-81b6-1c8db0a2d949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting R2.3 Step 2 for BERT (Hard Negative Mining) ---\n",
      "Loading Faiss index from 'fine_tuned_bert_articles.faiss'...\n",
      "✅ Faiss index loade|d from 'fine_tuned_bert_articles.faiss'\n",
      "\n",
      "Generating hard triplets (up to 5000)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mining Hard Negatives:  45%|████▌     | 56891/125634 [10:50<13:05, 87.48it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Reached maximum hard triplets: 5000\n",
      "✅ Model moved to GPU for second stage training.\n",
      "\n",
      "Starting second-stage fine-tuning for fine_tuned_minilm_second_stage...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     "
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='625' max='625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [625/625 03:59, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>5.274700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Second-stage fine-tuned model saved to 'fine_tuned_minilm_second_stage'\n",
      "\n",
      "Generating embeddings from second-stage fine-tuned BERT model: fine_tuned_minilm_second_stage\n",
      "\n",
      "Generating embeddings using sentence_transformer model from 'fine_tuned_minilm_second_stage'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1267/1267 [02:20<00:00,  9.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Embeddings generated. Shape: (40535, 768)\n",
      "\n",
      "Building Annoy index with 768 dimensions for 'fine_tuned_bert_second_stage'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding items to Annoy index: 100%|██████████| 40535/40535 [00:02<00:00, 18966.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Annoy index built and saved to 'fine_tuned_bert_second_stage_articles.ann'\n",
      "✅ Faiss index built and saved to 'fine_tuned_bert_second_stage_articles.faiss'\n",
      "--- R2.3 Step 2 for BERT Completed ---\n"
     ]
    }
   ],
   "source": [
    "# Updated BERT Second Stage Call\n",
    "print(\"\\n--- Starting R2.3 Step 2 for BERT (Hard Negative Mining) ---\")\n",
    "\n",
    "bert_model_for_hnm = load_sentence_transformer_model(\"fine_tuned_bert\")\n",
    "bert_faiss_index = load_faiss_index(\"fine_tuned_bert_articles.faiss\")\n",
    "\n",
    "news_dict_for_hnm = dict(zip(news_df_actual_train['news_id'], news_df_actual_train['text']))\n",
    "\n",
    "bert_hard_triplets = generate_hard_triplets(\n",
    "    faiss_index=bert_faiss_index,\n",
    "    embedding_model=bert_model_for_hnm,\n",
    "    tokenizer=None,\n",
    "    news_texts_dict=news_dict_for_hnm,\n",
    "    behaviors_df_train=behaviors_df_actual_train,\n",
    "    max_hard_triplets=5000\n",
    ")\n",
    "\n",
    "if bert_hard_triplets:\n",
    "    bert_second_stage_results = train_second_stage_triplet_model( # Capture results\n",
    "        model_instance=bert_model_for_hnm,\n",
    "        hard_triplets=bert_hard_triplets,\n",
    "        model_output_path_suffix=\"second_stage\",\n",
    "        news_df_val=news_df_val,\n",
    "        behaviors_df_val=behaviors_df_val,\n",
    "        epochs=1\n",
    "    )\n",
    "\n",
    "    # Plotting second-stage results for BERT\n",
    "    plot_training_metrics(bert_second_stage_results, \"BERT Second Stage Fine-tuning\")\n",
    "\n",
    "    # ... (rest of BERT second stage: generate embeddings and rebuild indices) ...\n",
    "\n",
    "    # After second stage, generate new embeddings from the *second-stage fine-tuned model*\n",
    "    print(f\"\\nGenerating embeddings from second-stage fine-tuned BERT model: {bert_second_stage_model_path}\")\n",
    "    second_stage_bert_embeddings, _ = generate_article_embeddings(\n",
    "        news_texts_for_embedding,\n",
    "        bert_second_stage_model_path,\n",
    "        \"sentence_transformer\"\n",
    "    )\n",
    "\n",
    "    # Re-build Annoy and Faiss indices with the *second-stage* embeddings\n",
    "    build_annoy_index(second_stage_bert_embeddings, \"fine_tuned_bert_second_stage\")\n",
    "    build_faiss_index_modular(second_stage_bert_embeddings, \"fine_tuned_bert_second_stage\")\n",
    "    print(\"--- R2.3 Step 2 for BERT Completed ---\")\n",
    "else:\n",
    "    print(\"No hard triplets found for BERT. Skipping second-stage fine-tuning.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee6a747-f7e5-4ff8-a472-639c63a8f31a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting R2.3 Step 2 for DeepSeek (Hard Negative Mining) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Faiss index from 'deepseek_peft_triplet_articles.faiss'...\n",
      "✅ Faiss index loade|d from 'deepseek_peft_triplet_articles.faiss'\n",
      "\n",
      "Generating hard triplets (up to 2000)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mining Hard Negatives: 100%|██████████| 125634/125634 [00:04<00:00, 25214.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Generated 0 hard triplets.\n",
      "No hard triplets found for DeepSeek. Skipping second-stage fine-tuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Updated DeepSeek Second Stage Call\n",
    "print(\"\\n--- Starting R2.3 Step 2 for DeepSeek (Hard Negative Mining) ---\")\n",
    "\n",
    "deepseek_model_for_hnm, deepseek_tokenizer_for_hnm = load_deepseek_model_for_embedding(\"./deepseek_peft_triplet\")\n",
    "deepseek_faiss_index = load_faiss_index(\"deepseek_peft_triplet_articles.faiss\")\n",
    "\n",
    "news_dict_for_hnm = dict(zip(news_df_actual_train['news_id'], news_df_actual_train['text']))\n",
    "\n",
    "deepseek_hard_triplets_input_examples = generate_hard_triplets(\n",
    "    faiss_index=deepseek_faiss_index,\n",
    "    embedding_model=deepseek_model_for_hnm,\n",
    "    tokenizer=deepseek_tokenizer_for_hnm,\n",
    "    news_texts_dict=news_dict_for_hnm,\n",
    "    behaviors_df_train=behaviors_df_actual_train,\n",
    "    max_hard_triplets=2000\n",
    ")\n",
    "\n",
    "if deepseek_hard_triplets_input_examples:\n",
    "    deepseek_second_stage_results = train_second_stage_deepseek( # Capture results\n",
    "        model_instance=deepseek_model_for_hnm,\n",
    "        tokenizer_instance=deepseek_tokenizer_for_hnm,\n",
    "        hard_triplets=deepseek_hard_triplets_input_examples,\n",
    "        model_output_path_suffix=\"second_stage\",\n",
    "        news_df_val=news_df_val, # Pass validation data\n",
    "        behaviors_df_val=behaviors_df_val, # Pass validation data\n",
    "        epochs=1 # Using 1 epoch for second stage as a starting point\n",
    "    )\n",
    "\n",
    "    # Plotting second-stage results for DeepSeek\n",
    "    plot_training_metrics(deepseek_second_stage_results, \"DeepSeek Second Stage Fine-tuning\")\n",
    "\n",
    "    # ... (rest of DeepSeek second stage: generate embeddings and rebuild indices) ...\n",
    "\n",
    "    # After second stage, generate new embeddings from the *second-stage fine-tuned model*\n",
    "    print(f\"\\nGenerating embeddings from second-stage fine-tuned DeepSeek model: {second_stage_deepseek_model_path}\")\n",
    "    second_stage_deepseek_embeddings, _ = generate_article_embeddings(\n",
    "        news_texts_for_embedding, # This already refers to news_df_train text\n",
    "        second_stage_deepseek_model_path, # Use the path to the second-stage model\n",
    "        \"deepseek\"\n",
    "    )\n",
    "\n",
    "    # Re-build Annoy and Faiss indices with the *second-stage* embeddings\n",
    "    build_annoy_index(second_stage_deepseek_embeddings, \"deepseek_peft_triplet_second_stage\")\n",
    "    build_faiss_index_modular(second_stage_deepseek_embeddings, \"deepseek_peft_triplet_second_stage\")\n",
    "    print(\"--- R2.3 Step 2 for DeepSeek Completed ---\")\n",
    "else:\n",
    "    print(\"No hard triplets found for DeepSeek. Skipping second-stage fine-tuning.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a61a158",
   "metadata": {},
   "source": [
    "## **Add Embeddings Visualization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec72636",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE # Ensure imported globally\n",
    "from sklearn.decomposition import PCA # Ensure imported globally\n",
    "\n",
    "print(\"\\n--- Embeddings Visualization (t-SNE/PCA) ---\")\n",
    "\n",
    "# Choose one set of embeddings to visualize (e.g., from the final DeepSeek model)\n",
    "# You might want to load the final DeepSeek embeddings if they were saved,\n",
    "# or use the `second_stage_deepseek_embeddings` variable if it's still in scope.\n",
    "# For demonstration, let's assume `second_stage_deepseek_embeddings` holds the final embeddings\n",
    "# and `news_df_train` is the corresponding DataFrame.\n",
    "\n",
    "# Ensure the embeddings are numpy arrays\n",
    "if isinstance(second_stage_deepseek_embeddings, list):\n",
    "    embeddings_to_visualize = np.array(second_stage_deepseek_embeddings)\n",
    "else:\n",
    "    embeddings_to_visualize = second_stage_deepseek_embeddings\n",
    "\n",
    "# Subsample for faster visualization if dataset is large\n",
    "n_samples = min(2000, embeddings_to_visualize.shape[0])\n",
    "sample_indices = np.random.choice(embeddings_to_visualize.shape[0], n_samples, replace=False)\n",
    "sampled_embeddings = embeddings_to_visualize[sample_indices]\n",
    "sampled_news_df = news_df_train.iloc[sample_indices].reset_index(drop=True)\n",
    "\n",
    "# 1. PCA for quick linear dimensionality reduction\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "pca_components = pca.fit_transform(sampled_embeddings)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(pca_components[:, 0], pca_components[:, 1],\n",
    "                      c=pd.factorize(sampled_news_df['category'])[0],\n",
    "                      cmap='Spectral', s=10, alpha=0.6)\n",
    "plt.colorbar(scatter, ticks=range(len(sampled_news_df['category'].unique())),\n",
    "             label='Category', format=plt.FuncFormatter(lambda i, *args: sampled_news_df['category'].unique()[int(i)]))\n",
    "plt.title('Article Embeddings Visualization (PCA)')\n",
    "plt.xlabel('PCA Component 1')\n",
    "plt.ylabel('PCA Component 2')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# 2. t-SNE for non-linear dimensionality reduction (can be slow for many points)\n",
    "# If you have many categories, it might be hard to distinguish in the plot\n",
    "if n_samples > 50: # t-SNE works best on smaller sets\n",
    "    print(\"\\nApplying t-SNE (this might take a while)...\")\n",
    "    tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "    tsne_components = tsne.fit_transform(sampled_embeddings)\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    scatter = plt.scatter(tsne_components[:, 0], tsne_components[:, 1],\n",
    "                          c=pd.factorize(sampled_news_df['category'])[0],\n",
    "                          cmap='Spectral', s=10, alpha=0.6)\n",
    "    plt.colorbar(scatter, ticks=range(len(sampled_news_df['category'].unique())),\n",
    "                 label='Category', format=plt.FuncFormatter(lambda i, *args: sampled_news_df['category'].unique()[int(i)]))\n",
    "    plt.title('Article Embeddings Visualization (t-SNE)')\n",
    "    plt.xlabel('t-SNE Component 1')\n",
    "    plt.ylabel('t-SNE Component 2')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Skipping t-SNE visualization due to small sample size.\")\n",
    "\n",
    "print(\"--- Embeddings Visualization Completed ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc02d7e4",
   "metadata": {},
   "source": [
    "## **RAG Implementatoin**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a9f6d96-d10a-47b9-9a7a-97434be826a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_faiss_index(embeddings):\n",
    "    faiss.normalize_L2(embeddings)\n",
    "    index = faiss.IndexFlatIP(embeddings.shape[1])\n",
    "    index.add(embeddings)\n",
    "    return index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "66eff836",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_docs(query, model, index, corpus, top_k=5):\n",
    "    query_embedding = model.encode([query], convert_to_numpy=True)\n",
    "    faiss.normalize_L2(query_embedding)\n",
    "    _, indices = index.search(query_embedding, top_k)\n",
    "    return [corpus[i] for i in indices[0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1cfab817",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rag_prompt(query, docs):\n",
    "    context = \"\\n\\n\".join(docs)\n",
    "    return f\"Context:\\n{context}\\n\\nQuestion: {query}\\nAnswer:\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3c1dbabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(prompt, model_path, max_tokens=150):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path, trust_remote_code=True).to(\"cuda\")\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    output = model.generate(**inputs, max_new_tokens=max_tokens, do_sample=False)\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f031c2b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches:   2%|▏         | 33/1584 [00:46<36:06,  1.40s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m texts \u001b[38;5;241m=\u001b[39m news_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m      3\u001b[0m embedding_model \u001b[38;5;241m=\u001b[39m SentenceTransformer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfine_tuned_mind_model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[43membedding_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_to_numpy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m index \u001b[38;5;241m=\u001b[39m build_faiss_index(embeddings)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Query example[\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\ARS-Project\\lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:720\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[1;34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings, **kwargs)\u001b[0m\n\u001b[0;32m    718\u001b[0m             \u001b[38;5;66;03m# fixes for #522 and #487 to avoid oom problems on gpu with large datasets\u001b[39;00m\n\u001b[0;32m    719\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m convert_to_numpy:\n\u001b[1;32m--> 720\u001b[0m                 embeddings \u001b[38;5;241m=\u001b[39m \u001b[43membeddings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    722\u001b[0m         all_embeddings\u001b[38;5;241m.\u001b[39mextend(embeddings)\n\u001b[0;32m    724\u001b[0m all_embeddings \u001b[38;5;241m=\u001b[39m [all_embeddings[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m np\u001b[38;5;241m.\u001b[39margsort(length_sorted_idx)]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Build once\n",
    "texts = news_df[\"text\"].tolist()\n",
    "embedding_model = SentenceTransformer(\"fine_tuned_mind_model\", device=\"cuda\")\n",
    "embeddings = embedding_model.encode(texts, convert_to_numpy=True, show_progress_bar=True)\n",
    "index = build_faiss_index(embeddings)\n",
    "\n",
    "# Query example[\n",
    "query = \"What happened in the recent elections?\"\n",
    "\n",
    "# RAG-style call\n",
    "docs = retrieve_docs(query, embedding_model, index, texts, top_k=5)\n",
    "prompt = build_rag_prompt(query, docs)\n",
    "answer = generate_answer(prompt, model_path=\"./deepseek_peft_triplet\")\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9132f558-3424-4093-962c-1b12a7b0217d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
